{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GygpZbSZF56r"
   },
   "source": [
    "# Quickstart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iAzSqH_1WbZ3"
   },
   "source": [
    "[Colab link](https://colab.research.google.com/drive/1ANPzed5n5yXJCOwpfa9x_uaYRRZVEitY?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F0IB0J-9F56s"
   },
   "source": [
    "The Goodfire SDK provides a powerful way to steer your AI models by changing the way they work internally. To do this we use mechanistic interpretability to find human-interpretable features and alter their activations. In this quickstart you'll learn how to:\n",
    "\n",
    "- Sample from a language model (in this case Llama 3 8B)\n",
    "\n",
    "- Search for interesting features and intervene on them to steer the model\n",
    "\n",
    "- Find features by contrastive search\n",
    "\n",
    "- Save and load Llama models with steering applied\n",
    "\n",
    "\n",
    "To get started, install our SDK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-lmNwVLtF56t",
    "outputId": "df51a673-4cec-4030-f0d0-a35e22cee008"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: goodfire in c:\\users\\danca\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.2.25)\n",
      "Requirement already satisfied: httpx<0.28.0,>=0.27.2 in c:\\users\\danca\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from goodfire) (0.27.2)\n",
      "Requirement already satisfied: ipywidgets<9.0.0,>=8.1.5 in c:\\users\\danca\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from goodfire) (8.1.5)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.4 in c:\\users\\danca\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from goodfire) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.9.2 in c:\\users\\danca\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from goodfire) (2.10.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\danca\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<0.28.0,>=0.27.2->goodfire) (4.6.2.post1)\n",
      "Requirement already satisfied: certifi in c:\\users\\danca\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<0.28.0,>=0.27.2->goodfire) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\danca\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<0.28.0,>=0.27.2->goodfire) (1.0.7)\n",
      "Requirement already satisfied: idna in c:\\users\\danca\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<0.28.0,>=0.27.2->goodfire) (3.10)\n",
      "Requirement already satisfied: sniffio in c:\\users\\danca\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<0.28.0,>=0.27.2->goodfire) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\danca\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.2->goodfire) (0.14.0)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\danca\\appdata\\roaming\\python\\python312\\site-packages (from ipywidgets<9.0.0,>=8.1.5->goodfire) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\danca\\appdata\\roaming\\python\\python312\\site-packages (from ipywidgets<9.0.0,>=8.1.5->goodfire) (8.29.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\danca\\appdata\\roaming\\python\\python312\\site-packages (from ipywidgets<9.0.0,>=8.1.5->goodfire) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in c:\\users\\danca\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ipywidgets<9.0.0,>=8.1.5->goodfire) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in c:\\users\\danca\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ipywidgets<9.0.0,>=8.1.5->goodfire) (3.0.13)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\danca\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3.0.0,>=2.9.2->goodfire) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\danca\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3.0.0,>=2.9.2->goodfire) (2.27.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\danca\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3.0.0,>=2.9.2->goodfire) (4.12.2)\n",
      "Requirement already satisfied: decorator in c:\\users\\danca\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets<9.0.0,>=8.1.5->goodfire) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\danca\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets<9.0.0,>=8.1.5->goodfire) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\danca\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets<9.0.0,>=8.1.5->goodfire) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\danca\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets<9.0.0,>=8.1.5->goodfire) (3.0.48)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\danca\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets<9.0.0,>=8.1.5->goodfire) (2.18.0)\n",
      "Requirement already satisfied: stack-data in c:\\users\\danca\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets<9.0.0,>=8.1.5->goodfire) (0.6.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\danca\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets<9.0.0,>=8.1.5->goodfire) (0.4.6)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\danca\\appdata\\roaming\\python\\python312\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets<9.0.0,>=8.1.5->goodfire) (0.8.4)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\danca\\appdata\\roaming\\python\\python312\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets<9.0.0,>=8.1.5->goodfire) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\danca\\appdata\\roaming\\python\\python312\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets<9.0.0,>=8.1.5->goodfire) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\danca\\appdata\\roaming\\python\\python312\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets<9.0.0,>=8.1.5->goodfire) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\danca\\appdata\\roaming\\python\\python312\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets<9.0.0,>=8.1.5->goodfire) (0.2.3)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\danca\\appdata\\roaming\\python\\python312\\site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets<9.0.0,>=8.1.5->goodfire) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install goodfire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in c:\\users\\danca\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "GOODFIRE_API_KEY = os.getenv('GOODFIRE_API_KEY')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FjOS55IMoIIj"
   },
   "outputs": [],
   "source": [
    "#from google.colab import userdata\n",
    "\n",
    "# Add your Goodfire API Key to your Colab secrets\n",
    "#GOODFIRE_API_KEY = userdata.get('GOODFIRE_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R9NhBTfxF56u"
   },
   "source": [
    "## Initialize the SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "TE9wY2GkF56u"
   },
   "outputs": [],
   "source": [
    "import goodfire\n",
    "\n",
    "client = goodfire.Client(\n",
    "    GOODFIRE_API_KEY\n",
    "  )\n",
    "\n",
    "# Instantiate a model variant\n",
    "variant = goodfire.Variant(\"meta-llama/Meta-Llama-3-8B-Instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1mxRiYh4F56u"
   },
   "source": [
    "You can get an API key through [our platform](https://platform.goodfire.ai). Reach out to the support channel or [contact@goodfire.ai](mailto:contact@goodfire.ai) if you need help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "91a29oN7F56u"
   },
   "source": [
    "## Replace model calls with OpenAI compatible API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yMvZOmJVF56v"
   },
   "source": [
    "Our sampling API is 'OpenAI-plus': we conform to the standard message format, plus some powerful additional steering features that we'll show you how to use below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M69LoGkLF56v",
    "outputId": "e61f934e-e5ce-42b4-d8fd-96022677318a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello there! Not much is going on, just waiting to help you with whatever you need. How about you? What's on your mind today?"
     ]
    }
   ],
   "source": [
    "for token in client.chat.completions.create(\n",
    "    [\n",
    "        {\"role\": \"user\", \"content\": \"hi, what is going on?\"}\n",
    "    ],\n",
    "    model=variant,\n",
    "    stream=True,\n",
    "    max_completion_tokens=50,\n",
    "):\n",
    "    print(token.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W912-UyVF56w"
   },
   "source": [
    "## Search for features and curate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_cY6ll9F56w"
   },
   "source": [
    "Before we can use features to steer the model, we first need to find some. The search endpoint takes an input string which describes the kind of feature you're looking for - in this case, features related to pirates. The `top_k` argument determines how many features will be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "73cuNdwMF56w",
    "outputId": "51086d91-a5fa-4c2a-e65e-563184fe2d4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeatureGroup([\n",
      "   0: \"Pirate-related language and themes\",\n",
      "   1: \"Pirate characters and themes in fiction and role-playing games\",\n",
      "   2: \"The model should roleplay as a pirate\",\n",
      "   3: \"Mischievous behavior and troublemaking\",\n",
      "   4: \"Mentions of rum, especially in pirate or cocktail contexts\"\n",
      "])\n",
      "[0.5869783759117126, 0.5635083913803101, 0.550840437412262, 0.4005439579486847, 0.3884296119213104]\n"
     ]
    }
   ],
   "source": [
    "pirate_features, relevance = client.features.search(\n",
    "    \"pirate\",\n",
    "    model=variant,\n",
    "    top_k=5\n",
    ")\n",
    "print(pirate_features)\n",
    "print(relevance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DlzkxqzpF56w",
    "outputId": "f12deada-962b-473f-fcde-4e387b30a301"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Feature(\"Pirate-related language and themes\")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "picked_pirate_feature = pirate_features[0]\n",
    "picked_pirate_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-SBeZoH4F56x"
   },
   "source": [
    "## Create a Variant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dTILwKfTF56x"
   },
   "source": [
    "Now we have a feature, we can set that feature's value. Setting a feature's value corresponds to reaching inside the model and turning up the variable corresponding to pirates in fictional settings. This will make the model talk like a pirate. Like prompting, steering isn't yet an exact science (but we're working on it!), so it's worth trying several features and using the model once you've applied some feature changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LvK1XjHUF56x",
    "outputId": "ebede47c-e53e-4dbc-c4cc-a4c5a40dd6de"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variant(\n",
       "   base_model=meta-llama/Meta-Llama-3-8B-Instruct,\n",
       "   edits={\n",
       "      Feature(\"Pirate-related language and themes\"): {'mode': 'nudge', 'value': 0.75},\n",
       "   }\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variant.reset()\n",
    "variant.set(picked_pirate_feature, 0.75) # -1 to 1 range, typically recommend starting around 0.5, -0.3\n",
    "# You can set additional feature interventions\n",
    "variant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kjN53NR_F56x"
   },
   "source": [
    "Let's unpack what's going on here:\n",
    "\n",
    "- A `Variant` is a language model with some steering behaviour applied. In this case, we've steered a pirate feature.\n",
    "\n",
    "- We refer to these steering behaviours as `edits`. This is a list of features that have been changed from the values they'd take in the original model.\n",
    "\n",
    "- The feature steering details `{'mode': 'nudge', 'value': 0.8}` tells what kind of steering has been applied. The default is to `nudge` a feature to some value: this biases the feature activation by some specified amount."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DV_pHFbpF56x"
   },
   "source": [
    "### Enjoy your new model variant!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TT9loQwzF56y"
   },
   "source": [
    "Now we can chat with the model variant by passing our new `Variant` to the chat completion - we originally had `model=\"meta-llama/Meta-Llama-3-8B-Instruct\"`, now we have `model=variant`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nT6HOXtxF56y",
    "outputId": "b2d04d99-1671-419d-d1f2-49c0038c577b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! *pirate voice* Ahoy, matey! I'm doing great, thanks for asking! I'm a helpful pirate assistant, here to help you with all your questions and adventures! What can I help you with today?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for token in client.chat.completions.create(\n",
    "    [\n",
    "        {\"role\": \"user\", \"content\": \"Hello. How are you?\"}\n",
    "    ],\n",
    "    model=variant,\n",
    "    stream=True,\n",
    "    max_completion_tokens=50,\n",
    "):\n",
    "    print(token.choices[0].delta.content, end=\"\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HPW5EeW_F56y"
   },
   "source": [
    "## Use contrastive features to fine-tune with a single example!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EYuHJadyF56y"
   },
   "source": [
    "We can also find features to steer with in a data-driven way. This lets us create new model variants instantly with a single example. To find features, we use the `contrast` endpoint. This is a little more complex, but very powerful.\n",
    "\n",
    "Contrastive search starts with two chat datasets. In `dataset_1` we give examples of behaviour we want to steer away from. In `dataset_2`, we give examples of the kind of behaviour we want to elicit. These examples are paired: the first example in `dataset_1` is contrasted with the first example in `dataset_2`, and so on.\n",
    "\n",
    "We found that contrastive search often produced relevant features, but a naive implementation also produces a lot of spurious ones. We reduce this issue by providing a short description of what we're trying to achieve in the `dataset_1_rerank_query` argument (and `dataset_2_rerank_query`). This description reranks the results of the contrastive search, which surfaces far more relevant features.\n",
    "\n",
    "Both of these steps are important: the contrastive search ensures that the features are mechanistically useful, and the reranking step makes finding the kind of behaviour you want in the list easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GLmNYkUFF56y",
    "outputId": "f44f7b6d-63b2-44ab-c587-dcb6f8ec39ac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeatureGroup([\n",
       "   0: \"The model is telling a joke or offering to tell one\",\n",
       "   1: \"Repetitive joke patterns, especially involving common objects or animals\",\n",
       "   2: \"The user's turn to speak in a conversation\",\n",
       "   3: \"The user has posed a riddle or puzzle to be solved\",\n",
       "   4: \"The user is requesting entertaining or interesting content\"\n",
       "])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variant.reset()\n",
    "\n",
    "_, comedic_features = client.features.contrast(\n",
    "    dataset_1=[\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Hello how are you?\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"I am a helpful assistant. How can I help you?\"\n",
    "            }\n",
    "        ]\n",
    "    ],\n",
    "    dataset_2=[\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Hello how are you?\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\":\n",
    "                  \"What do you call an alligator in a vest? An investigator.\"\n",
    "            }\n",
    "        ],\n",
    "    ],\n",
    "    dataset_2_feature_rerank_query=\"comedy\",\n",
    "    model=variant,\n",
    "    top_k=5\n",
    ")\n",
    "\n",
    "comedic_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uK3JZ_CiF56z"
   },
   "source": [
    "We now have lists of features to add and remove. Let's add some plausible-looking ones from `to_add`. We can set multiple features at once and then sample from the new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "pW3Z6HfIF56z"
   },
   "outputs": [],
   "source": [
    "variant.set(comedic_features[0], 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XY46sEUQF56z",
    "outputId": "bedf07cf-bea5-43c6-e892-669bf998235e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A classic one! Why did the moon go to the doctor? Because it was feeling a little \"spacey\"! Okay, okay, I'll stop with the moon puns. Seriously, the moon is Earth's only natural satellite, and it's a pretty big deal. It's about 2,000 miles (3,200 km) away from us, and it takes about 28 days to orbit the Earth. That's why we have a lunar cycle, get it? Okay, I'll stop."
     ]
    }
   ],
   "source": [
    "for token in client.chat.completions.create(\n",
    "    [\n",
    "        {\"role\": \"user\", \"content\": \"Hello. Tell me about the moon.\"}\n",
    "    ],\n",
    "    model=variant,\n",
    "    stream=True,\n",
    "    max_completion_tokens=200,\n",
    "):\n",
    "    print(token.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K_PjkuUpF562"
   },
   "source": [
    "## Saving and loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mPLKAhKaF563"
   },
   "source": [
    "You can also persist model variants to use later and give your model variants a name to help you remember what they do. Each variant has an associated unique ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "rUQ6OXclF563",
    "outputId": "e05d9461-5847-4f43-e96c-de527788b262"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'db68072c-e35c-49f2-ad72-204fff7410e0'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variant_id = client.variants.create(variant, \"This model got jokes\")\n",
    "variant_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MkAVcPiMF563"
   },
   "source": [
    "You can also get a list of all of your model variants (these are shared per organisation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jNsHNrJ_F563",
    "outputId": "ef133313-4503-45bf-de34-08250904e6f2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[VariantMetaData(name='This model got jokes', base_model='meta-llama/Meta-Llama-3-8B-Instruct', id='db68072c-e35c-49f2-ad72-204fff7410e0')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variants = client.variants.list()\n",
    "variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BrKOQKeeF563"
   },
   "source": [
    "Using `variants.get` lets you pull a model you've previously saved with `variants.create` and sample from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0IWYfIl9F564",
    "outputId": "c1794ab1-7c68-4437-bea2-cf4143c74070"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variant(\n",
       "   base_model=meta-llama/Meta-Llama-3-8B-Instruct,\n",
       "   edits={\n",
       "      Feature(\"The model is telling a joke or offering to tell one\"): {'mode': 'nudge', 'value': 0.5},\n",
       "   }\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = client.variants.get(variant_id)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lK0kGx3ZF564",
    "outputId": "11fe8151-2d27-4192-a7ba-7b0920933ce6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin-tastic! Why did the whale go to the ocean party? Because it was a whale of a time! Okay, okay, I'll stop with the bad jokes.\n",
      "\n",
      "But seriously, did you know that whales are actually mammals? Yeah, I know, it's a blows-ful pun! Okay, okay, I'll stop.\n",
      "\n",
      "On a more serious note, whales are actually really smart and social creatures. They communicate with each other using clicks, whistles, and even songs! And did you know that some whales can live up to 100 years in the wild? That's a long fin-tastic life!\n",
      "\n",
      "I hope that made a splash with you!"
     ]
    }
   ],
   "source": [
    "for token in client.chat.completions.create(\n",
    "    [\n",
    "        {\"role\": \"user\", \"content\": \"Hello. Talk to me about the whales.\"}\n",
    "    ],\n",
    "    model=model,\n",
    "    stream=True,\n",
    "    max_completion_tokens=200,\n",
    "):\n",
    "    print(token.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YXkBCgr2F564"
   },
   "source": [
    "### Update an existing Variant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yWxflMszF564"
   },
   "source": [
    "Model variants aren't static; we can make changes to their features and re-upload them, perhaps with a new name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "oJq4ojI2F565"
   },
   "outputs": [],
   "source": [
    "variant.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QryJxxETF565"
   },
   "source": [
    "Now we'll try and make an extremely unfunny model - one that couldn't tell a joke even if it tried."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qVjtR2O8F565",
    "outputId": "6dec6c81-90c6-4aef-ebc4-e4425c1681fb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeatureGroup([\n",
       "   0: \"The model is telling a joke or offering to tell one\",\n",
       "   1: \"Repetitive joke patterns, especially involving common objects or animals\",\n",
       "   2: \"The user's turn to speak in a conversation\",\n",
       "   3: \"The user has posed a riddle or puzzle to be solved\",\n",
       "   4: \"The user is requesting entertaining or interesting content\"\n",
       "])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variant.reset()\n",
    "\n",
    "_, comedic_features = client.features.contrast(\n",
    "    dataset_1=[\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Hello how are you?\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"I am a helpful assistant. How can I help you?\"\n",
    "            }\n",
    "        ]\n",
    "    ],\n",
    "    dataset_2=[\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Hello how are you?\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"What do you call an alligator in a vest? An investigator.\"\n",
    "            }\n",
    "        ],\n",
    "    ],\n",
    "    dataset_2_feature_rerank_query=\"comedy\",\n",
    "    model=variant,\n",
    "    top_k=5\n",
    ")\n",
    "\n",
    "comedic_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sS6RbLK9F565",
    "outputId": "ff9a9981-eaa3-4f9b-c514-f0781bbfb9e6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variant(\n",
       "   base_model=meta-llama/Meta-Llama-3-8B-Instruct,\n",
       "   edits={\n",
       "      Feature(\"The model is telling a joke or offering to tell one\"): {'mode': 'nudge', 'value': -0.4},\n",
       "      Feature(\"Repetitive joke patterns, especially involving common objects or animals\"): {'mode': 'nudge', 'value': -0.4},\n",
       "      Feature(\"The user is requesting entertaining or interesting content\"): {'mode': 'nudge', 'value': -0.4},\n",
       "   }\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variant.reset()\n",
    "variant.set(comedic_features[0,1,4], -0.4)\n",
    "variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vqiN8h_HF565",
    "outputId": "4710ab57-7b78-41a8-9ef8-6d90f52b1596"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'd be delighted to share a joke with you. Here's a fun one: \"What's the best way to make a wish come true? According to our joke, it's with a sprinkle of magic dust and a dash of good fortune."
     ]
    }
   ],
   "source": [
    "for token in client.chat.completions.create(\n",
    "    [\n",
    "        {\"role\": \"user\", \"content\": \"Hello. Tell me a joke.\"}\n",
    "    ],\n",
    "    model=variant,\n",
    "    stream=True,\n",
    "    max_completion_tokens=200,\n",
    "):\n",
    "    print(token.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OO71BdmNF566"
   },
   "source": [
    "As intended, no sense of humour whatsoever. We can update our model in the model repository, and change its name to reflect its missing sense of humour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "UQmwyua3F566"
   },
   "outputs": [],
   "source": [
    "client.variants.update(variant_id, model, new_name='Not so funny anymore, huh?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pi7DSCTrF566",
    "outputId": "ef267bd4-378f-4540-fe0d-0efedcbfcb4f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variant(\n",
       "   base_model=meta-llama/Meta-Llama-3-8B-Instruct,\n",
       "   edits={\n",
       "      Feature(\"The model is telling a joke or offering to tell one\"): {'mode': 'nudge', 'value': 0.5},\n",
       "   }\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.variants.get(variant_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ufOLb0bF566"
   },
   "source": [
    "### Delete a Variant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "50nXGmwJF566"
   },
   "source": [
    "Finally, you can delete variants you no longer need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "09-IkjzrF567",
    "outputId": "1c98eafe-4fa6-43dd-f596-f9980e202450"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for v in client.variants.list():\n",
    "    client.variants.delete(v.id)\n",
    "\n",
    "client.variants.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2rKdYjGwF56z"
   },
   "source": [
    "## Inspecting features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DCX9ma6EF56z"
   },
   "source": [
    "You can inspect what features are activating in a given conversation with the `inspect` API, which returns a `context` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-NPdJmEXF560",
    "outputId": "1389af89-8f03-43a6-9419-b2093f5bc3b2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ContextInspector(\n",
       "   <|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
       "   \n",
       "   Hola amigo<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
       "   \n",
       "   Hola!<|eot_id|>\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variant.reset()\n",
    "\n",
    "context = client.features.inspect(\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Hola amigo\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"Hola!\"\n",
    "        },\n",
    "    ],\n",
    "    model=variant,\n",
    ")\n",
    "context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DoLYHvpXF560"
   },
   "source": [
    "You can select the top `k` activating features ranked by activation strength."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "-cM9fzUWF560"
   },
   "outputs": [],
   "source": [
    "top_features = context.top(k=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D771aCfoZTSL"
   },
   "source": [
    "You can also output feature activations as a sparse vector to use in machine learning pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HeFGJ9meZMoc",
    "outputId": "ad61dcd5-3550-4643-c221-c9d78c9bc5f1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 0., 0., ..., 0., 0., 0.]),\n",
       " {28127: Feature(\"Spanish greeting 'Hola' triggering Spanish language responses\"),\n",
       "  40612: Feature(\"The model's turn to speak in multilingual conversations\"),\n",
       "  64861: Feature(\"End of model's response, user's turn to speak\"),\n",
       "  47867: Feature(\"The model's opening greeting and offer of help\"),\n",
       "  29884: Feature(\"The model's turn to speak in informal or roleplay conversations\")})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_vector, feature_lookup = top_features.vector()\n",
    "sparse_vector, feature_lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TlNmXqPzSmYg"
   },
   "source": [
    "For machine learning pipelines you can export the context as a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1nneSQ-sStRm",
    "outputId": "c3e4282b-85e3-4201-964f-3013af913a8e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix = context.matrix(return_lookup=False)\n",
    "\n",
    "matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bCEjsttkZXmH"
   },
   "source": [
    "You can also inspect individual tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Swch5Y9qF560",
    "outputId": "4eb190dc-5844-471c-c358-3dc8dfbf6dc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token(\"Hola\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FeatureActivations(\n",
       "   0: (Feature(\"Spanish greeting 'Hola' triggering Spanish language responses\"), 3.90625)\n",
       "   1: (Feature(\"The model's multilingual greeting responses\"), 3.84375)\n",
       "   2: (Feature(\"Informal, friendly conversation openers\"), 1.0546875)\n",
       "   3: (Feature(\"Conversation initiators and greetings across languages\"), 1.03125)\n",
       "   4: (Feature(\"The model's initial greeting (usually 'Hello')\"), 0.875)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(context.tokens[-3])\n",
    "\n",
    "token_acts = context.tokens[-3].inspect()\n",
    "token_acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3gayUpm-Zewk",
    "outputId": "dbb8c8ee-3d14-4402-944c-d483b26b7e08"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 0., 0., ..., 0., 0., 0.]),\n",
       " {28127: Feature(\"Spanish greeting 'Hola' triggering Spanish language responses\"),\n",
       "  47378: Feature(\"The model's multilingual greeting responses\"),\n",
       "  42620: Feature(\"Informal, friendly conversation openers\"),\n",
       "  3625: Feature(\"Conversation initiators and greetings across languages\"),\n",
       "  7352: Feature(\"The model's initial greeting (usually 'Hello')\")})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector, feature_lookup = token_acts.vector()\n",
    "\n",
    "vector, feature_lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZmXDKgfDF561"
   },
   "source": [
    "## Inspecting specific features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UyLHshwuF561"
   },
   "source": [
    "There may be specific features whose activation patterns you're interested in exploring. In this case, you can specify features such as *animal_features* and pass that into the `features` argument of `inspect`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QOmkHg4PF561",
    "outputId": "fce6c1d7-ebba-4daa-c92d-6e049378fe1b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeatureGroup([\n",
       "   0: \"Whales and their characteristics\",\n",
       "   1: \"Common animals, especially pets and familiar wild animals\",\n",
       "   2: \"Animal-related concepts and discussions\",\n",
       "   3: \"Animal characteristics and behaviors, especially mammals\",\n",
       "   4: \"Wildlife, especially in natural or conservation contexts\"\n",
       "])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "animal_features, _ = client.features.search(\"animals such as whales\", top_k=5)\n",
    "animal_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w4qvl2dSF561",
    "outputId": "56338fd9-4313-4e48-d6f5-2647a88292e2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ContextInspector(\n",
       "   <|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
       "   \n",
       "   Tell me about whales.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
       "   \n",
       "   Whales are cetaceans.<|eot_id|>\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = client.features.inspect(\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Tell me about whales.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"Whales are cetaceans.\"\n",
    "        },\n",
    "    ],\n",
    "    model=variant,\n",
    "    features=animal_features\n",
    ")\n",
    "context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "loIo4RItF561"
   },
   "source": [
    "Now you can retrieve the top k activating *animal features* in the `context`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4zWBcDjbF561",
    "outputId": "d8a3a45c-78c7-46f9-c635-beab19f1b8a5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeatureActivations(\n",
       "   0: (Feature(\"Whales and their characteristics\"), 2.4938151041666665)\n",
       "   1: (Feature(\"Wildlife, especially in natural or conservation contexts\"), 0.625)\n",
       "   2: (Feature(\"Animal characteristics and behaviors, especially mammals\"), 0)\n",
       "   3: (Feature(\"Animal-related concepts and discussions\"), 0)\n",
       "   4: (Feature(\"Common animals, especially pets and familiar wild animals\"), 0)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "animal_feature_acts = context.top(k=5)\n",
    "animal_feature_acts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P9yRcKdPF567"
   },
   "source": [
    "## Using OpenAI SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KxOy2yWhF567"
   },
   "source": [
    "You can also work directly with the OpenAI SDK for inference since our endpoint is fully compatible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kpSc2zGYF567",
    "outputId": "7f0a7445-49fb-46db-f277-d2bfd682d416"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.54.4)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.7.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.9.2)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.6)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sljn5ct5F567",
    "outputId": "08872b41-79ef-4b7c-ee98-540be8595619"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-58a0066b-84bf-43ee-9770-714d831336f5', choices=[Choice(finish_reason=None, index=0, logprobs=None, message=ChatCompletionMessage(content=\"I'm happy to help! However, I don't see anyone or anything mentioned in your question. Could you please provide more context or information about who or what you are referring to?\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1732250837, model='meta-llama/Meta-Llama-3-8B-Instruct', object='chat.completion', service_tier=None, system_fingerprint='fp_goodfire', usage=None)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Fetch saved variant w/ Goodfire client\n",
    "variant = client.variants.get(variant_id)\n",
    "\n",
    "oai_client = OpenAI(\n",
    "    api_key=GOODFIRE_API_KEY,\n",
    "    base_url=\"https://api.goodfire.ai/api/inference/v1\",\n",
    ")\n",
    "\n",
    "oai_client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"who is this\"},\n",
    "    ],\n",
    "    model=variant.base_model,\n",
    "    extra_body={\"controller\": variant.controller.json()},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SlNgWeELF567"
   },
   "source": [
    "### Next steps\n",
    "\n",
    "We've seen how to find human-interpretable features inside Llama 3, apply those features to steer the model behaviour, and surface feature groups using contrastive search. We've also covered saving, loading, and editing your model variants in your Goodfire model repo. This behaviour really only scratches the surface of what you can do with our tooling - there's a richer and more expressive model programming language you can learn about in our advanced tutorial `advanced.ipynb`."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
